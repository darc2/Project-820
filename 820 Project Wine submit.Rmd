---
title: "Project Wine 820"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


Section I : Introduction

            This report is done to illustrate how wine quality can be predicted by using knn, rf and nn.  It will also show what other algos have been used in other reports. A comparison will be drawn along with some recommendations.
            
Section II : Related Works

            

            insert other works here

Section III : Methodologies

    A. Data collection and exploration.

              The wine data set is publicly available from the data base of UCI. Here is               the link,            
              
              https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/ . 
              
              The report will focus on the winequality-white.csv dataset.

    B. Feature Reduction
    
              Which techniques where used to do feature reduction?

    C. Classification Model
              1. KNN
              2. RF
              3. NN
    
    D. Performance Measure Metrics
              1. accuracy
              2. training time - efficiency
                2a. compare before and after feature reduction
              3. stability - k-fold
              
    
    
Section IV : Results and Discussion

      Discuss the results of knn 

Section V : Conclusion and Future Work



```{r}

###  Load white wine dataset
wine <- read.csv("winequality-white.csv", header = T , stringsAsFactors = F, sep = ";", na.strings = c("","NA"))
# had to use sep = ";" since csv is not comma separated

wine_data_nn <- read.csv("winequality-white.csv", header = T , stringsAsFactors = F, sep = ";", na.strings = c("","NA"))
head(wine)
```

```{r}
str(wine)
```


### 

Add code here to show histograms

" A histogram is the appropriate graph for the initial exploration of a continuous variable.  By means of a set of vertical bars, it shows how the numerical values of that variable are distributed.  The histogram allows to calculate the probability of representation of any value value of the continuous variable under study, which is great importance if we want to make inferences and estimate population values from the results of our sample.

A histogram provides a visual representation of the distribution of a dataset: location, spread and skewness of data; it also helps to visualize whether the distribution is symmetric or skewed left or right.  In addition, if it is unimodal, bimodal or multimodal.  It can also show any outliers or gaps in the data.  In brief, a histogram summarizes the distribution properties of a continuous numerical variable. (Towards Data Science) "

from: www.towardsdatascience.com/histograms-why-how-431a5cfbfcd5 



```{r}
hist(wine$quality)
```

```{r}
hist(wine$fixed.acidity)
```

```{r}
hist(wine$volatile.acidity)
```

```{r}
hist(wine$citric.acid)
```

```{r}
hist(wine$residual.sugar)
```

```{r}
hist(wine$chlorides)
```

```{r}
hist(wine$free.sulfur.dioxide)
```

```{r}
hist(wine$total.sulfur.dioxide)
```

```{r}
hist(wine$density)
```

```{r}
hist(wine$pH)
```

```{r}
hist(wine$sulphates)
```

```{r}
hist(wine$alcohol)
```


```{r}
plot(wine)
```

```{r}

### Selected features that most people would recognize

#subset_wine_PCA <- wine[,c("fixed.acidity","volatile.acidity ","citric.acid", "residual.sugar","chlorides","free.sulfur.dioxide", "total.sulfur.dioxide", "density", "pH", "sulphates", "alcohol")]

subset_wine_PCA <- wine[,c("fixed.acidity","volatile.acidity","citric.acid", "residual.sugar","chlorides","free.sulfur.dioxide","total.sulfur.dioxide", "density", "pH", "sulphates", "alcohol")]

subset_wine_W <- wine[,c("fixed.acidity","residual.sugar","density", "pH", "sulphates", "alcohol", "quality")]

print(head(subset_wine_W))
```

```{r}
plot(subset_wine_W)
```

###############################################################################

Show correlation


```{r}
install.packages("corrplot")
library(corrplot)
```

```{r}
wine.cor <- cor(wine, method = "pearson")
```

```{r}
corrplot(wine.cor)
```

### include comments in the report... 
- We can see that alcohol is negatively correlated to density and that density is positively correlated to residual sugars.
- We also see that quality is positively correlated to alcohol.


```{r}
#subset_WW_ind <- subset_wine_W[,1:5]
#SS.WW.pca.rawdata <- prcomp(subset_WW_ind, scale = FALSE, center = FALSE)
#SS.WW.pca.rawdata


# PCA - Principal Component Analysis

# modify subset
wine_pca <- prcomp(subset_wine_W, scale. = FALSE, center = FALSE)
wine_pca
```
```{r}
#plot(SS.WW.pca.rawdata, type = "l", main = "without data normalization")

plot(wine_pca, type = "l", main = "without data normalization" )
```
```{r}
wine_pca_Full <- prcomp(wine, scale. = TRUE, center = TRUE)
wine_pca_Full
```

```{r}
plot(wine_pca_Full, type = "l", main = "Scaled and Centered " )
```

The line graph for PCA on the wine dataset shows that features 7 to 12 can be removed without losing information.

This is could work since Pearson correlation plot indicate that the four of first 6 features have at least some correlation with quality.

```{r}
plot(wine_pca_Full)
```
```{r}
wine_pca_for_Q <- prcomp(subset_wine_PCA, scale. = TRUE, center = TRUE)
wine_pca_for_Q
plot(wine_pca_for_Q, type = "l", main = "Scaled and Centered, no Q " )
```

From the scree plot, we can see that the inflection point happens at component #6.  Typical convention tells us that we can now discard from component #7 and on.  We will see how the models performed when we subset the data with only the first six features.


```{r}
#####################################################

#Turn quality field into factor
wine$quality <- as.factor(wine$quality)
```


```{r}

##################################################################################
                            #    #     #     #    #     #
# knn starts below          #  #       # #   #    # #   #
                            # #        #  #  #    #  #  #
                            #  #       #   # #    #   # #
                            #    #     #     #    #     #
###################################################################################
```


```{r}
##Generate a random number that is 90% of the total number of rows in dataset.
 #ran <- sample(1:nrow(iris), 0.9 * nrow(iris)) 

set.seed(22)
wine_ran <- sample(1:nrow(wine), 0.9 * nrow(wine))
 
 ##the normalization function is created
 nor <-function(x) { (x -min(x))/(max(x)-min(x))   }
 
```


```{r}
##Run normalization on first 11 columns of data set because they are the predictors

# Normalization ensures that all features are mapped to the same range of values. The values are "normalized to be from 0.0 to 1.0".  Standardization means that the range of value are "standardized" to measure how many staandard deviations the value is from its mean.

wine_knn_norm <- as.data.frame(lapply(wine[,c(1,2,3,4,5,6,7,8,9,10,11)], nor))
 
summary(wine_knn_norm)
```


```{r}
##extract training set
#iris_train <- iris_norm[ran,]

# use system.time to get the length of time it takes to run a function
wine_knn_train <- wine_knn_norm[wine_ran,]
wine_rf_train <- wine[wine_ran,]
```


```{r}
head(wine_knn_train)
```


```{r}
##extract testing set

wine_knn_test <- wine_knn_norm[-wine_ran,]
wine_rf_test <- wine[-wine_ran,]
```


```{r}

##extract 12th column of train dataset because it will be used as 'cl' argument in knn function.
wine_knn_target_category <- wine[wine_ran, 12]
wine_rf_target_category <- wine[wine_ran, 12]
head(wine_knn_target_category)
str(wine_knn_target_category)
```


```{r}

##extract 12th column of "test" dataset to measure the accuracy
wine_knn_test_category <- wine[-wine_ran, 12]
wine_rf_test_category <- wine[-wine_ran, 12]
```


```{r}
##load the package class
 library(class)
```

```{r}
str(wine_knn_train)
```
```{r}
str(wine_knn_test)
```
```{r}
str(wine_knn_target_category)
```

```{r}

#system.time added to measure model's running time

knn_time <- system.time(wine_knn_model <- knn(wine_knn_train, wine_knn_test, cl=wine_knn_target_category, k=13 ))

knn_time

```
```{r}
knn_time_5 <- system.time(wine_knn_model_5 <- knn(wine_knn_train, wine_knn_test, cl=wine_knn_target_category, k=5 ))

knn_time_5 
```

```{r}
install.packages('e1071', dependencies = TRUE)
library(caret)
```
```{r}
##create confusion matrix
 #tab <- table(pr,iris_test_category)

wine_knn_Conf_Matrix <- table(wine_knn_model,wine_knn_test_category)
wine_knn_Conf_Matrix_5<- table(wine_knn_model_5,wine_knn_test_category)
```

```{r}
confusionMatrix(wine_knn_Conf_Matrix)
```
```{r}
confusionMatrix(wine_knn_Conf_Matrix_5)
```

```{r}
##this function divides the correct predictions by total number of predictions that tell us how accurate the model is.
 
# this is if we don't use the confusionMatrix function from library caret.

 accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
 #accuracy(tab)
 accuracy(wine_knn_Conf_Matrix)
 
## [1] 80

```
```{r}
 accuracy(wine_knn_Conf_Matrix_5)
```


```{r}
head(wine_knn_test)
```

```{r}
# KNN plus FE

# for lack of knowledge at the moment, we will apply the information from varImpPlot to remove some features for KNN

# we're removing Chlorides, citric acid and fixed acidity...
```


```{r}
wine_knn_test_category_fe <- wine_knn_test_category
wine_knn_test_category_pca <- wine_knn_test_category

```

```{r}
# subsetting for FE and PCA

wine_knn_train_fe <- wine_knn_train[,c( "volatile.acidity" , "residual.sugar" , "free.sulfur.dioxide" , "total.sulfur.dioxide", "density" , "pH",  "sulphates" , "alcohol")]

wine_knn_train_pca <- wine_knn_train[,c("fixed.acidity", "volatile.acidity" , "citric.acid",
                           "residual.sugar" , "chlorides" , "free.sulfur.dioxide")]

str(wine_knn_train_fe)
```


```{r}

wine_knn_test_fe <- wine_knn_test[,c( "volatile.acidity" , "residual.sugar" , "free.sulfur.dioxide" , "total.sulfur.dioxide", "density" , "pH",  "sulphates" , "alcohol")]

wine_knn_test_pca <- wine_knn_test[,c("fixed.acidity", "volatile.acidity" , "citric.acid" ,
                           "residual.sugar" , "chlorides" , "free.sulfur.dioxide")]

str(wine_knn_test_fe)
```


```{r}
wine_knn_target_category_fe <- wine_knn_target_category
wine_knn_target_category_pca <- wine_knn_target_category
str(wine_knn_target_category_fe)
```

```{r}
knn_time_fe <- system.time(wine_knn_model_fe <- knn(wine_knn_train_fe, wine_knn_test_fe, cl=wine_knn_target_category_fe, k=13 ))

knn_time_fe_5 <- system.time(wine_knn_model_fe_5 <- knn(wine_knn_train_fe, wine_knn_test_fe, cl=wine_knn_target_category_fe, k=5 ))
```


```{r}
knn_time_pca <- system.time(wine_knn_model_pca <- knn(wine_knn_train_pca, wine_knn_test_pca, cl=wine_knn_target_category_pca, k=13 ))

knn_time_pca_5 <- system.time(wine_knn_model_pca_5 <- knn(wine_knn_train_pca, wine_knn_test_pca, cl=wine_knn_target_category_pca, k=5 ))

```

```{r}
knn_time_fe
knn_time_fe_5
knn_time_pca
knn_time_pca_5
```
```{r}
##create confusion matrix
 #tab <- table(pr,iris_test_category)

wine_knn_Conf_Matrix_fe <- table(wine_knn_model_fe,wine_knn_test_category_fe)
wine_knn_Conf_Matrix_fe_5 <- table(wine_knn_model_fe_5,wine_knn_test_category_fe)

wine_knn_Conf_Matrix_pca <- table(wine_knn_model_pca,wine_knn_test_category_pca)
wine_knn_Conf_Matrix_pca_5 <- table(wine_knn_model_pca_5,wine_knn_test_category_pca)
```

```{r}
wine_knn_Conf_Matrix_fe
```

```{r}
class(wine_knn_model_fe)
```


```{r}
#wine_knn_model_fe_num <- as.numeric(wine_knn_model_fe)
#class(wine_knn_model_fe_num)
```

```{r}
class(wine_knn_test_category_fe)
```
```{r}
#wine_knn_test_category_fe_num <- as.numeric(wine_knn_test_category_fe)
#class(wine_knn_test_category_fe_num)
```

```{r}
# confusionMatrix function needs both fields to be in the same class, ie. factor or integer
confusionMatrix(wine_knn_Conf_Matrix_fe)
```
```{r}
confusionMatrix(wine_knn_Conf_Matrix_fe_5)
```

Stats for KNN with FE

accuracy for knn , k = 13 : 52.04
accuracy for knn with fe, k = 13 : 52.86

accuracy for knn , k = 5 : 55.10
accuracy for knn with fe, k = 5 : 51.22



Compare statistics by class and note whether it improved after feature engineering.


knn

Statistics by Class:

                     Class: 3 Class: 4 Class: 5 Class: 6 Class: 7 Class: 8 Class: 9
Sensitivity          0.000000  0.00000   0.5764   0.7037  0.42697 0.047619       NA
Specificity          1.000000  1.00000   0.8410   0.5803  0.89027 0.995736        1
Pos Pred Value            NaN      NaN   0.6014   0.5693  0.46341 0.333333       NA

knn with fe 

Sensitivity          0.000000  0.00000   0.5278   0.6481  0.40449 0.047619       NA
Specificity          1.000000  1.00000   0.8064   0.5474  0.89526 0.991471        1
Pos Pred Value            NaN      NaN   0.5315   0.5303  0.46154 0.200000       NA

In R, Recall = Sensitivity and Precision = Pos Pred Value.  Comparing the classes with available data, these values were better before feature engineering was applied.  This implies that all of the features in the dataset should be used when applying the KNN model. Another reason could be that since this dataset has been used in a previous report, it must have gone through some feature engineering or dimension reduction before it was shared.

```{r}
confusionMatrix(wine_knn_Conf_Matrix_pca)
```


```{r}
confusionMatrix(wine_knn_Conf_Matrix_pca_5)
```

```{r}

# Test area to figure out why overall precision and recall does not show up in results. Only category specific precision and recall metrics are shown.

#wine_knn_Conf_Matrix_fe_num <- table(wine_knn_model_fe_num, wine_knn_test_category_fe_num)
#confusionMatrix(wine_knn_Conf_Matrix_fe_num)
#wine_knn_Conf_Matrix_fe_num
```

```{r}
# K-fold for KNN starts below
```


```{r}
# code trControl grid method here


#trControl <- trainControl(method = "cv",
#    number = 10,
#    search = "grid")

#trControl_ran <- trainControl(method = "cv",
#    number = 10,
#    search = "random")

```

```{r}
# code trControl search method here
```

```{r}
# verify knn data for training... might need to use data with quality in it
```

```{r}
#set.seed(1)
#rf_time_cv_grid <- system.time(rf_default_grid <- train(quality~.,
#                    data = wine_rf_train,
#                    method = "rf",
#                    metric = "Accuracy",
#                    trControl = trControl))

#print(rf_default_grid)
#rf_time_cv_grid

#set.seed(1)
#knn_time_cv_grid <- system.time(knn_default_grid <- train(quality~.,
#                    data = wine_rf_train, #change to knn train data
#                    method = "knn",
#                    metric = "Accuracy",
#                    trControl = trControl))

#print(rf_default_grid)
#rf_time_cv_grid


```


```{r}

################################################################################

              ####      ######    
              #   #     #           
              ###       ####         
              #  #      #           
              #   #     #    

###############################################################################


# Random Forest starts below


library(randomForest)
library(ggplot2) # Data visualization
# library(readr) # CSV file I/O, e.g. the read_csv function
```

```{r}
# Train and Test sets for Random forest model
wine_rf_train <- wine[wine_ran,]
#wine_rf_train_norm <- wine_knn_norm[wine_ran,]

wine_rf_test <- wine[-wine_ran,]
#wine_rf_test_norm <- wine_knn_norm[-wine_ran,]

#head(wine_rf_test_norm)
```

```{r}
str(wine_rf_test)
#str(wine_rf_test_norm)
```

```{r}
###
#Create random forest model with 1000 trees and 7 random variables

rf_time<- system.time(model <- randomForest(quality ~ ., data = wine, importance=TRUE, proximity=TRUE, ntree=1000, mtry=7))

rf_time
model

```
```{r}
plot(model)
```


```{r}

#Check out importance variables in order to find which features are important.

varImpPlot(model)

# The most important features are likely to appear closer to the root of the tree, while less important features will often appear close to the leaves.

```
These two were first shown 
  MSE = mean square error(?)

  Node Purity - find out what this means

After subsequent runs, these two showed up

    Mean Decrease Accuracy 
          - is also (%IncMSE) 
          - This shows how much our model accuracy decreases if we                                         leave out that variable
  
    Mean Decrease Gini
          - is also (IncNodePurity)
          - This is a measure of variable importance based on Gini impurity index used for                 calculating the splits in trees


    The higher the value of mean decrease accuracy or mean decrease gini, the higher the importance of the variable to our model
    
    
    Since Fixed.acidity, chlorides and citric acid are in the bottom half of both measures. I chose to remove these features.  

from Prof Erdem,

top 3 features are important and the fact that both graphs show it means it's really important

use and check performance : top 6 features and knee points and overlap ... 


```{r}
#  Random forest code with cross validation using k-folds starts below

library(randomForest)
library(caret)
library(e1071)
```


```{r}
# k-folds with k = 10

trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

trControl_ran <- trainControl(method = "cv",
    number = 10,
    search = "random")

```

train(formula, df, method = "rf", metric= "Accuracy", trControl = trainControl(), tuneGrid = NULL)
argument
- `formula`: Define the formula of the algorithm
- `method`: Define which model to train. Note, at the end of the tutorial, there is a list of all the models that can be trained
- `metric` = "Accuracy": Define how to select the optimal model
- `trControl = trainControl()`: Define the control parameters
- `tuneGrid = NULL`: Return a data frame with all the possible combination


```{r}

set.seed(1)
rf_time_cv_grid <- system.time(rf_default_grid <- train(quality~.,
                    data = wine_rf_train,
                    method = "rf",
                    metric = "Accuracy",
                    trControl = trControl))

print(rf_default_grid)

rf_time_cv_grid

```
The best accuracy which is equal to 69.34 %  where mtry = 2 

The algorithm used 500 trees as a default and tested 3 different values of mtry : 2, 6 11.




```{r}

# random option for trControl
set.seed(1)
rf_time_cv_search_500 <- system.time(rf_default_search_500 <- train(quality~.,
                    data = wine_rf_train,
                    method = "rf",
                    metric = "Accuracy",
                    trControl = trControl_ran,
                    ))

rf_default_search_500

```

Using the random search method for train control, the best accuracy is equal to 69.01 % where mtry = 1.  The accuracy is slightly worse than grid method.  

The algorithm used 500 trees as a default and tested 3 different values of mtry : 1, 2, 7.

The algorithm used 113.97 seconds to compute. A full 14 seconds faster than grid method.


```{r}
rf_time_cv_search_500
```


```{r}
set.seed(1)
rf_time_cv_grid_1000 <- system.time(rf_default_grid_1000 <- train(quality~.,
                    data = wine_rf_train,
                    method = "rf",
                    metric = "Accuracy",
                    trControl = trControl,
                    ntree = 1000))
```

```{r}
rf_time_cv_grid_1000
```

```{r}
print(rf_default_grid_1000)
```
Using the grid method for train control and setting ntree = 1000, the best accuracy which is equal to 69.28 %  where mtry = 2.   

The algorithm used 1000 trees and tested 3 different values of mtry : 2, 6 11. It also required about 253 seconds vs 128 for ntree = 500.

The increase in ntree value did not improve the accuracy of the model

Next, we add tuneGrid to see if we can get better accuracy:


```{r}
set.seed(1)
tuneGrid <- expand.grid(.mtry = c(1: 10))
rf_time_cv_mtry <- system.time(rf_mtry <- train(quality~.,
    data = wine_rf_train,
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tuneGrid,
    trControl = trControl,
    ntree = 500))
```

```{r}
print(rf_mtry)

rf_time_cv_mtry
```

Adding tunegrid and setting mtry to use 1:10 and setting ntree = 500, the best accuracy which is equal to 69.37 %  where mtry = 1.   

The algorithm used 500 trees and tested 10 different values of mtry. It also required about 398 seconds vs 128 for the default setting of 3 mtry values.

The addition of tune did improve the accuracy of the model for this dataset by a very small increase of 0.03 percent over the default settings.

With the least run time of 127.7 seconds and the most accuracy at 69.34%,  rf_default_grid with default settings for random forest will be used in evaluating the model.


```{r}
# model evaluation

# using the predict function in the library caret:

# Here, rf_default_grid model used for test dataset (wine_rf_test).

prediction_rf <- predict(rf_default_grid, wine_rf_test)
```


```{r}
prediction_rf_model <- predict(model, wine_rf_test)

```

```{r}
# using confusionMatrix to get accuracy.

confusionMatrix(prediction_rf, wine_rf_test$quality)
```

The model produced an accuracy of 72.45 %. This is better than the other training sets' accuracy.

Why? Needs more research and reading!

```{r}
confusionMatrix(prediction_rf_model, wine_rf_test$quality)
```


```{r}
# do Random Forest with FE

# use rf_default_grid model with subset data that removed fixed acidity , ...

#subset_wine_W <- wine_W[,c("fixed.acidity","residual.sugar", "pH", "sulphates", "alcohol", "quality")]

wine_rf_train_fe <- wine_rf_train[,c( "volatile.acidity" , "residual.sugar" , "free.sulfur.dioxide" , "total.sulfur.dioxide", "density" , "pH",  "sulphates" , "alcohol", "quality")]

wine_rf_train_pca <- wine_rf_train[,c( "volatile.acidity" , "residual.sugar" , "free.sulfur.dioxide" , "total.sulfur.dioxide", "density" , "pH",  "sulphates" , "alcohol", "quality")]

set.seed(1)
rf_time_cv_grid_fe <- system.time(rf_default_grid_fe <- train(quality~.,
                    data = wine_rf_train_fe, # replace with fe subset
                    method = "rf",
                    metric = "Accuracy",
                    trControl = trControl))
```


```{r}
print(rf_default_grid_fe)
rf_time_cv_grid_fe

```

After applying feature engineering by removing fixed.acidity , chlorides and citric.acid, the accuracy declined to 67.88 %.  Accuracy is 69.10 % without feature engineering.

```{r}
system.time(prediction_rf_fe <- predict(rf_default_grid_fe, wine_rf_test))
```

```{r}
confusionMatrix(prediction_rf_fe, wine_rf_test$quality)
```


72.86 % before FE

72.65 % after FE

```{r}
rf_time_cv_grid

rf_time_cv_grid_fe
```

Even though FE improves run time by 23 seconds, the cost of slightly lower accuracy is not a worth it.


plot run times for each RF model iteration for presentation purposes.

place all run times to a vector and plot

rf_time_plot <- c[rf_time,rf_time_cv_grid, rf_time_cv_grid_1000]


```{r}
# create data.frame to hold all run time values

rf_rt_grid_500 <- data.frame("grid 500",
                             rf_time_cv_grid["elapsed"],
                             rf_default_grid$results$Accuracy[1])
names(rf_rt_grid_500) <- c("RF", "Time", "Accuracy")
rf_run_time_df2 <- rbind(rf_rt_grid_500)

rf_rt_grid_1000 <- data.frame("grid 1000",
                              rf_time_cv_grid_1000["elapsed"],
                              rf_default_grid_1000$results$Accuracy[1])
names(rf_rt_grid_1000) <- c("RF", "Time", "Accuracy")
rf_run_time_df2 <- rbind(rf_run_time_df2, rf_rt_grid_1000)

rf_rt_search_500 <- data.frame("search 500",
                               rf_time_cv_search_500["elapsed"],
                               rf_default_search_500$results$Accuracy[1])
names(rf_rt_search_500) <- c("RF", "Time", "Accuracy")
rf_run_time_df2 <- rbind(rf_run_time_df2, rf_rt_search_500)

rf_rt_mtry <- data.frame("mtry",
                         rf_time_cv_mtry["elapsed"], 
                         rf_mtry$results$Accuracy[5])
names(rf_rt_mtry) <- c("RF", "Time", "Accuracy")
rf_run_time_df2 <- rbind(rf_run_time_df2, rf_rt_mtry)

rf_rt_grid_fe <- data.frame("grid fe",
                            rf_time_cv_grid_fe["elapsed"],
                            rf_default_grid_fe$results$Accuracy[1])
names(rf_rt_grid_fe) <- c("RF", "Time", "Accuracy")
rf_run_time_df2 <- rbind(rf_run_time_df2, rf_rt_grid_fe)

#rf_run_time_df1 <- as.data.frame(rf_run_time_df1)
```


```{r}
#str(rf_default)

#rf_default$results$Accuracy[1]
```

```{r}
str(rf_run_time_df2)
#rf_run_time_df["elapsed"]

rf_run_time_df2

# now add accuracy from each iteration
```
```{r}

ggplot() +
        geom_col(data = rf_run_time_df2,
       aes( x=RF, y = Time), colour = 'blue')

```
Default grid search with feature engineering, grid_fe, took only 105 seconds with an accuracy of 67.86%.  Compared with default grid search without feature engineering's run time of 128 second with an accuracy of 69.10%. The time penalty 23 seconds might be a good investment to gain more than 1% accuracy.

```{r}
ggplot() +
        geom_col(data = rf_run_time_df2,
       aes( x=RF, y = Accuracy), colour = 'red')
```

As the graph for accuracy shows, there is very little difference from one RF model to the next.  The next factor to consider when using the RF model is the run time.  
```{r}
#hist(rf_run_time_df1)
```

```{r}
### Why normalize?  

#Normalize data between 0 and 1
norm <- function(x) {(x - min(x, na.rm=TRUE))/(max(x,na.rm=TRUE) -
min(x, na.rm=TRUE))}
wine$fixed.acidity <- norm(wine$fixed.acidity)
wine$volatile.acidity <- norm(wine$volatile.acidity)
wine$citric.acid <- norm(wine$citric.acid)
wine$residual.sugar <- norm(wine$residual.sugar)
wine$chlorides <- norm(wine$chlorides)
wine$free.sulfur.dioxide <- norm(wine$free.sulfur.dioxide)
wine$total.sulfur.dioxide <- norm(wine$total.sulfur.dioxide)
wine$density <- norm(wine$density)
wine$pH <- norm(wine$pH)
wine$sulphates <- norm(wine$sulphates)
wine$alcohol <- norm(wine$alcohol)

```



```{r}
################################################################################

# NN starts here               #     #    #     #
                               # #   #    # #   #
                               #  #  #    #  #  #
                               #   # #    #   # #
                               #     #    #     #

################################################################################

# NN starts here
# solving classification problems with neuralnet
# https://datascienceplus.com/neuralnet-train-and-test-neural-networks-using-r/

#Neural Network
install.packages(neuralnet) 
# seems like we just need to call the library(neuralnet)

library(neuralnet)
#library(nnet)

```


```{r}
# Training and Test Data

# The data is normalized for Neural Networks just like it is for KNN. Among the best practices for training a neural network is to normalize your data. Generally, this speeds up learning and leads to faster convergence.

wine_nn_norm <- as.data.frame(lapply(wine_data_nn[,c(1,2,3,4,5,6,7,8,9,10,11,12)], norm))
 
#trainset <- maxmindf[1:160, ]
#testset <- maxmindf[161:200, ]

#wine_knn_train <- wine_knn_norm[wine_ran,]
# wine_ran in line 251 , randomizer
wine_nn_train <- wine_nn_norm[wine_ran,]

##extract testing set
 #iris_test <- iris_norm[-ran,]

#wine_knn_test <- wine_knn_norm[-wine_ran,]
wine_nn_test <- wine_nn_norm[-wine_ran,]
```


```{r}
# Multiclass Classification

nn_time <- system.time(wine_nn <- neuralnet(quality ~ fixed.acidity+ volatile.acidity + citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, data = wine_nn_train, hidden=c(2,1), linear.output=FALSE, threshold=0.01))


#wine_nn <- nnet(quality ~ fixed.acidity+ volatile.acidity + citric.acid + residual.sugar + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, data = wine_nn_train, size = 2, hidden=c(2,1), linear.output=FALSE, threshold=0.01)

```
```{r}
#print(wine_nn)
nn_time
```


```{r}
#nn$result.matrix

wine_nn$result.matrix
```


Comparison

NN

error                             3.244118e+01
reached.threshold                 9.449625e-03
steps                             5.138000e+03


NN_FE

error                             3.294836e+01
reached.threshold                 9.937215e-03
steps                             2.399000e+04


```{r}
#plot(nn)
plot(wine_nn)
```

Testing the accuracy of the NN model.

The subset function is used to eliminate the dependent variable from the test data.

The compute function then creates the prediction variable.

A results variable then compares the predicted data with actual data.

A confusion matrix is then created with the table function.


```{r}
#Test the resulting output
#temp_test <- subset(testset, select = c("fcfps","earnings_growth", "de", "mcap", "current_ratio"))

temp_nn_test <- subset(wine_nn_test, select = 
                         c("fixed.acidity", "volatile.acidity" , "citric.acid" ,
                           "residual.sugar" , "chlorides" , "free.sulfur.dioxide" ,
                           "total.sulfur.dioxide", "density" , "pH",  "sulphates" ,
                           "alcohol"))
#head(temp_nn_test)
#head(temp_test)
```

```{r}

wine_nn_results <- compute(wine_nn, temp_nn_test)

# wine_nn_results_fe <- compute(wine_nn_fe, temp_nn_test_fe)
# checking code to compare function call for nn computation
# str(wine_nn_results)
```

```{r}
#results <- data.frame(actual = testset$dividend, prediction = nn.results$net.result)

nn.results <- data.frame(actual = wine_nn_test$quality, prediction = wine_nn_results$net.result )

#nn.results
```
 

```{r}

rounded_nn_results <- sapply(nn.results, round, digits=0)

#roundedresultsdf=data.frame(roundedresults)

rounded_nn_results_df = data.frame(rounded_nn_results)

nn_tab <- table(rounded_nn_results_df$actual, rounded_nn_results_df$prediction)

nn_tab
```


```{r}
#head(rounded_nn_results_df)
#str(rounded_nn_results_df)
```

```{r}
#install.packages('e1071', dependencies = TRUE)
#library(caret)
```


```{r}
#confusionMatrix(rounded_nn_results_df)
confusionMatrix(nn_tab)
```

For Neural Network (NN) without FE

        Accuracy : 0.6878 
     Sensitivity : 0.9296 --- > Recall         
     Specificity : 0.3544          
  Pos Pred Value : 0.6650 --- > Precision

 
            Reference
Predicted   Event   No Event
   Event      A       B
No Event      C       D


Precision = A / (A+B)

Recall = A / (A+C)

Sensitivity = A / (A+C)

Specificity = D / (B+D)

```{r}
################################################################################

# NN starts here               #     #    #     #     ###### ######
# Feature Engineering          # #   #    # #   #.    #.     #
                               #  #  #    #  #  #.    ###.   #####
                               #   # #    #   # #.    #.     #
                               #     #    #     #.    #.     ######

################################################################################
```


```{r}
# Multiclass Classification

#    Remove Chlorides, citric acid and fixed acidity...
#    in Random Forest, these features were in the lowest ranking of importance 
#    in both Mean Decrease Accuracy and Mean Decrease Gini

nn_time_fe <- system.time(wine_nn_fe <- neuralnet(quality ~ 
                                                    volatile.acidity + 
                                                    residual.sugar + 
                                                    free.sulfur.dioxide + 
                                                    total.sulfur.dioxide + 
                                                    density + 
                                                    pH +
                                                    sulphates + 
                                                    alcohol, 
                                                  data = wine_nn_train, 
                                                  hidden=c(2,1), linear.output=FALSE,
                                                  threshold=0.01))

nn_time_pca <- system.time(wine_nn_pca <- neuralnet(quality ~
                                                     fixed.acidity + 
                                                     volatile.acidity + 
                                                     citric.acid +
                                                     residual.sugar + 
                                                     chlorides + 
                                                     free.sulfur.dioxide,
                                                   data = wine_nn_train, 
                                                   hidden=c(2,1), 
                                                   linear.output=FALSE, threshold=0.01))

nn_time_fe
```

```{r}
nn_time_pca
```

```{r}
#system.time(wine_nn_fe <- neuralnet(quality ~ volatile.acidity + residual.sugar + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, data = wine_nn_train, hidden=c(2,1), linear.output=FALSE, threshold=0.01))
```

```{r}
#plot(nn)
plot(wine_nn_fe)
```
```{r}
plot(wine_nn_pca)
```

```{r}
wine_nn_fe$result.matrix
```
```{r}
wine_nn_pca$result.matrix
```


```{r}
#Test the resulting output

# temp_nn_test <- subset(wine_nn_test, select = 
#                         c("fixed.acidity", "volatile.acidity" , "citric.acid" ,
#                           "residual.sugar" , "chlorides" , "free.sulfur.dioxide" ,
#                           "total.sulfur.dioxide", "density" , "pH",  "sulphates" ,
#                           "alcohol"))

temp_nn_test_fe <- subset(wine_nn_test, select = 
                            c( "volatile.acidity", "residual.sugar","free.sulfur.dioxide",
                               "total.sulfur.dioxide", "density", "pH", "sulphates",
                               "alcohol"))

temp_nn_test_pca <- subset(wine_nn_test, select = 
                        c("fixed.acidity", "volatile.acidity" , "citric.acid" ,
                           "residual.sugar" , "chlorides" , "free.sulfur.dioxide"))

```

```{r}
#Test the resulting output

wine_nn_results_fe <- compute(wine_nn_fe, temp_nn_test_fe)
wine_nn_results_pca <- compute(wine_nn_pca, temp_nn_test_pca)
```

```{r}

nn.results_fe <- data.frame(actual = wine_nn_test$quality, prediction = wine_nn_results_fe$net.result )

nn.results_pca <- data.frame(actual = wine_nn_test$quality, prediction = wine_nn_results_pca$net.result )

#nn.results_fe
```

```{r}
#roundedresults<-sapply(results,round,digits=0)

rounded_nn_results_fe <- sapply(nn.results_fe, round, digits=0)
rounded_nn_results_pca <- sapply(nn.results_pca, round, digits=0)
#roundedresultsdf=data.frame(roundedresults)

rounded_nn_results_df_fe = data.frame(rounded_nn_results_fe)
rounded_nn_results_df_pca = data.frame(rounded_nn_results_pca)

nn_tab_fe <- table(rounded_nn_results_df$actual, rounded_nn_results_df_fe$prediction)
nn_tab_pca <- table(rounded_nn_results_df$actual, rounded_nn_results_df_pca$prediction)

nn_tab_fe

nn_tab_pca
```

```{r}
confusionMatrix(nn_tab_fe)
```

  
  For Neural Network (NN) without FE , the metrics for accuracy, recall and precision remained the same.  This is rather expected and suspicious at the same time. Especially since the values are exactly identical.  The VarImp function applied for feature engineering is expected to have little to no effect to the model other than improve the computational complexity.

        Accuracy : 0.6878 
     Sensitivity : 0.9296 --- > Recall         
     Specificity : 0.3544          
  Pos Pred Value : 0.6650 --- > Precision
 
 
```{r}
confusionMatrix(nn_tab_pca)
```
 
 Now for Neural Network model with PCA applied, the performance metrics declined.
 This is somewhat expected as well.  The reason being that the features were not selected by their correlation to the target class.  In future revisions of this work, applying VarImp to the dataset then applying PCA might improve the performance metric.
 

                      NN                  NN_FE             NN_FE

          Accuracy : 0.6878             0.6878            0.6306
       Sensitivity : 0.8913          
       Specificity : 0.2944          
    Pos Pred Value : 0.6196  
